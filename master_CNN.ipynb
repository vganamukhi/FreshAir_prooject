{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import keras\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import boto3\n",
    "import urllib.request\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90487,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd \"C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\data\"\n",
    "X = pkl.load(open(\"x_master.p\", \"rb\"))\n",
    "Y = pkl.load(open(\"y_master.p\", \"rb\"))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin=[]\n",
    "y_bin=[]\n",
    "smoking_event=0\n",
    "non_smoking_event=0\n",
    "\n",
    "\n",
    "for i, y in enumerate(Y):\n",
    "  if X[i].shape == (160, 4):\n",
    "      if y in [1,2,3]:\n",
    "        y_bin.append(1)\n",
    "        x_bin.append(X[i])\n",
    "        smoking_event+=1\n",
    "      elif y >= 20:\n",
    "        y_bin.append(0)\n",
    "        x_bin.append(X[i])\n",
    "        non_smoking_event+=1\n",
    "        \n",
    "x_bin=np.array(x_bin)\n",
    "y_bin=np.array(y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83369, 160, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75032, 160, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_bin, y_bin, test_size=0.1, random_state = 99)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2485"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "np.count_nonzero(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75032 samples, validate on 8337 samples\n",
      "Epoch 1/500\n",
      "75032/75032 [==============================] - 55s 736us/sample - loss: 0.6164 - acc: 0.8551 - val_loss: 0.3144 - val_acc: 0.8827\n",
      "Epoch 2/500\n",
      "75032/75032 [==============================] - 52s 695us/sample - loss: 0.4412 - acc: 0.8968 - val_loss: 0.3575 - val_acc: 0.8625\n",
      "Epoch 3/500\n",
      "75032/75032 [==============================] - 54s 714us/sample - loss: 0.3755 - acc: 0.9142 - val_loss: 0.2376 - val_acc: 0.9225\n",
      "Epoch 4/500\n",
      "75032/75032 [==============================] - 54s 715us/sample - loss: 0.3348 - acc: 0.9250 - val_loss: 0.2043 - val_acc: 0.9387\n",
      "Epoch 5/500\n",
      "75032/75032 [==============================] - 54s 716us/sample - loss: 0.2944 - acc: 0.9354 - val_loss: 0.3416 - val_acc: 0.8745\n",
      "Epoch 6/500\n",
      "75032/75032 [==============================] - 54s 717us/sample - loss: 0.2722 - acc: 0.9381 - val_loss: 0.2492 - val_acc: 0.9121\n",
      "Epoch 7/500\n",
      "75032/75032 [==============================] - 55s 732us/sample - loss: 0.2538 - acc: 0.9429 - val_loss: 0.2636 - val_acc: 0.9022\n",
      "Epoch 8/500\n",
      "75032/75032 [==============================] - 55s 730us/sample - loss: 0.2327 - acc: 0.9473 - val_loss: 0.2236 - val_acc: 0.9391\n",
      "Epoch 9/500\n",
      "75032/75032 [==============================] - 55s 730us/sample - loss: 0.2116 - acc: 0.9523 - val_loss: 0.2321 - val_acc: 0.9370\n",
      "Epoch 10/500\n",
      "75032/75032 [==============================] - 54s 722us/sample - loss: 0.1993 - acc: 0.9538 - val_loss: 0.2299 - val_acc: 0.9361\n",
      "Epoch 11/500\n",
      "75032/75032 [==============================] - 54s 717us/sample - loss: 0.1951 - acc: 0.9566 - val_loss: 0.2026 - val_acc: 0.9328\n",
      "Epoch 12/500\n",
      "75032/75032 [==============================] - 53s 711us/sample - loss: 0.1808 - acc: 0.9586 - val_loss: 0.1791 - val_acc: 0.9687\n",
      "Epoch 13/500\n",
      "75032/75032 [==============================] - 54s 714us/sample - loss: 0.1740 - acc: 0.9611 - val_loss: 0.1600 - val_acc: 0.9572\n",
      "Epoch 14/500\n",
      "75032/75032 [==============================] - 54s 726us/sample - loss: 0.1641 - acc: 0.9623 - val_loss: 0.1728 - val_acc: 0.9632\n",
      "Epoch 15/500\n",
      "75032/75032 [==============================] - 51s 681us/sample - loss: 0.1631 - acc: 0.9638 - val_loss: 0.2469 - val_acc: 0.9278\n",
      "Epoch 16/500\n",
      "22016/75032 [=======>......................] - ETA: 35s - loss: 0.1389 - acc: 0.9676"
     ]
    }
   ],
   "source": [
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=.0001, patience=10, verbose=0, mode='auto',\n",
    "                         baseline=None, restore_best_weights=False)\n",
    "class_weight = {0: 1, 1: 30}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64 ,kernel_size=2,kernel_initializer = 'he_normal' ,activation='relu', \n",
    "                 input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Conv1D(filters=64 ,kernel_size=2,kernel_initializer = 'he_normal', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer = 'he_normal', activation= 'elu'))\n",
    "model.add(Dense(1 ,kernel_initializer = 'he_normal', activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(x_bin, y_bin, batch_size=32,epochs=500,validation_split = .1, callbacks = [early_stop], \n",
    "                   class_weight = class_weight)\n",
    "#model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      8031\n",
      "           1       0.29      0.95      0.44       306\n",
      "\n",
      "    accuracy                           0.91      8337\n",
      "   macro avg       0.64      0.93      0.70      8337\n",
      "weighted avg       0.97      0.91      0.93      8337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y  = model.predict_classes(x_test)\n",
    "print(classification_report(y_test,pred_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\data\\test_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13362,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd \"C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\data\\test_data\"\n",
    "x_test1 = pkl.load(open(\"x_test.p\", \"rb\"))\n",
    "y_test1 = pkl.load(open(\"y_test.p\", \"rb\"))\n",
    "\n",
    "x_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\data\\test_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12362,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd \"C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\data\\test_data\"\n",
    "x_test2 = pkl.load(open(\"x_test1911.p\", \"rb\"))\n",
    "y_test2 = pkl.load(open(\"y_test1911.p\", \"rb\"))\n",
    "\n",
    "x_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10908, 160, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=[]\n",
    "y_test=[]\n",
    "smoking_event=0\n",
    "non_smoking_event=0\n",
    "\n",
    "\n",
    "for i, y in enumerate(y_test2):\n",
    "  if x_test2[i].shape == (160, 4):\n",
    "      if y in [1,2,3]:\n",
    "        y_test.append(1)\n",
    "        x_test.append(x_test2[i])\n",
    "        smoking_event+=1\n",
    "      elif y >= 20:\n",
    "        y_test.append(0)\n",
    "        x_test.append(x_test2[i])\n",
    "        non_smoking_event+=1\n",
    "        \n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\CNN model\\models\n",
      "WARNING:tensorflow:From C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\eric_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     10591\n",
      "           1       0.53      0.91      0.67      1064\n",
      "\n",
      "    accuracy                           0.92     11655\n",
      "   macro avg       0.76      0.91      0.81     11655\n",
      "weighted avg       0.95      0.92      0.93     11655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\CNN model\\models\"\n",
    "best_model = tf.keras.models.load_model('model-21-0.9510.hdf5')\n",
    "pred_y  = best_model.predict_classes(x_test)\n",
    "print(classification_report(y_test,pred_y))\n",
    "#1910 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\CNN model\\models\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     10188\n",
      "           1       0.49      0.90      0.63       720\n",
      "\n",
      "    accuracy                           0.93     10908\n",
      "   macro avg       0.74      0.92      0.80     10908\n",
      "weighted avg       0.96      0.93      0.94     10908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\CNN model\\models\"\n",
    "best_model = tf.keras.models.load_model('model-21-0.9510.hdf5')\n",
    "pred_y  = best_model.predict_classes(x_test)\n",
    "print(classification_report(y_test,pred_y))\n",
    "#1911 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layers_norm</th>\n",
       "      <th>Layers_filter</th>\n",
       "      <th>num_filters</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>initialization</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>percision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.946962</td>\n",
       "      <td>0.954060</td>\n",
       "      <td>0.421129</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.583751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.940832</td>\n",
       "      <td>0.944384</td>\n",
       "      <td>0.368957</td>\n",
       "      <td>0.947712</td>\n",
       "      <td>0.531136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.940432</td>\n",
       "      <td>0.944130</td>\n",
       "      <td>0.353733</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.514693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.958822</td>\n",
       "      <td>0.964255</td>\n",
       "      <td>0.460292</td>\n",
       "      <td>0.928105</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.963886</td>\n",
       "      <td>0.970639</td>\n",
       "      <td>0.522472</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.664286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.966285</td>\n",
       "      <td>0.970426</td>\n",
       "      <td>0.497297</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.641115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.964152</td>\n",
       "      <td>0.970919</td>\n",
       "      <td>0.512104</td>\n",
       "      <td>0.898693</td>\n",
       "      <td>0.652432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.960954</td>\n",
       "      <td>0.967161</td>\n",
       "      <td>0.461149</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.608018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.961087</td>\n",
       "      <td>0.970013</td>\n",
       "      <td>0.478799</td>\n",
       "      <td>0.885621</td>\n",
       "      <td>0.621560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.972148</td>\n",
       "      <td>0.977796</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.962420</td>\n",
       "      <td>0.971399</td>\n",
       "      <td>0.500938</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.636472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.962820</td>\n",
       "      <td>0.971812</td>\n",
       "      <td>0.482633</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.618992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.958955</td>\n",
       "      <td>0.966814</td>\n",
       "      <td>0.454231</td>\n",
       "      <td>0.859477</td>\n",
       "      <td>0.594350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.967351</td>\n",
       "      <td>0.974331</td>\n",
       "      <td>0.512671</td>\n",
       "      <td>0.859477</td>\n",
       "      <td>0.642247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.957489</td>\n",
       "      <td>0.963442</td>\n",
       "      <td>0.483456</td>\n",
       "      <td>0.859477</td>\n",
       "      <td>0.618824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.972148</td>\n",
       "      <td>0.980875</td>\n",
       "      <td>0.587444</td>\n",
       "      <td>0.856209</td>\n",
       "      <td>0.696809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.969216</td>\n",
       "      <td>0.977490</td>\n",
       "      <td>0.551797</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.670090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.969883</td>\n",
       "      <td>0.979555</td>\n",
       "      <td>0.579418</td>\n",
       "      <td>0.846405</td>\n",
       "      <td>0.687915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.970282</td>\n",
       "      <td>0.980915</td>\n",
       "      <td>0.582766</td>\n",
       "      <td>0.839869</td>\n",
       "      <td>0.688086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.966018</td>\n",
       "      <td>0.973398</td>\n",
       "      <td>0.520576</td>\n",
       "      <td>0.826797</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.968150</td>\n",
       "      <td>0.977996</td>\n",
       "      <td>0.534783</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.642298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.975880</td>\n",
       "      <td>0.985993</td>\n",
       "      <td>0.624021</td>\n",
       "      <td>0.781046</td>\n",
       "      <td>0.693759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.972681</td>\n",
       "      <td>0.980968</td>\n",
       "      <td>0.565632</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.653793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>0.977745</td>\n",
       "      <td>0.990951</td>\n",
       "      <td>0.695266</td>\n",
       "      <td>0.767974</td>\n",
       "      <td>0.729814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>0.976146</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.638587</td>\n",
       "      <td>0.767974</td>\n",
       "      <td>0.697329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Layers_norm  Layers_filter  num_filters  num_nodes initialization  \\\n",
       "21          2.0            3.0        128.0       64.0      he_normal   \n",
       "22          3.0            3.0        128.0       64.0     he_uniform   \n",
       "7           3.0            3.0         64.0       64.0      he_normal   \n",
       "2           3.0            2.0         64.0       64.0     he_uniform   \n",
       "15          3.0            3.0         64.0      128.0      he_normal   \n",
       "6           3.0            3.0         64.0       64.0     he_uniform   \n",
       "14          3.0            3.0         64.0      128.0     he_uniform   \n",
       "4           2.0            3.0         64.0       64.0     he_uniform   \n",
       "8           2.0            2.0         64.0      128.0     he_uniform   \n",
       "13          2.0            3.0         64.0      128.0      he_normal   \n",
       "5           2.0            3.0         64.0       64.0      he_normal   \n",
       "17          2.0            2.0        128.0       64.0      he_normal   \n",
       "10          3.0            2.0         64.0      128.0     he_uniform   \n",
       "18          3.0            2.0        128.0       64.0     he_uniform   \n",
       "23          3.0            3.0        128.0       64.0      he_normal   \n",
       "12          2.0            3.0         64.0      128.0     he_uniform   \n",
       "20          2.0            3.0        128.0       64.0     he_uniform   \n",
       "19          3.0            2.0        128.0       64.0      he_normal   \n",
       "16          2.0            2.0        128.0       64.0     he_uniform   \n",
       "3           3.0            2.0         64.0       64.0      he_normal   \n",
       "11          3.0            2.0         64.0      128.0      he_normal   \n",
       "0           2.0            2.0         64.0       64.0     he_uniform   \n",
       "1           2.0            2.0         64.0       64.0      he_normal   \n",
       "9           2.0            2.0         64.0      128.0      he_normal   \n",
       "24          2.0            2.0        128.0      128.0     he_uniform   \n",
       "\n",
       "     val_acc  train_acc  percision    recall        f1  \n",
       "21  0.946962   0.954060   0.421129  0.950980  0.583751  \n",
       "22  0.940832   0.944384   0.368957  0.947712  0.531136  \n",
       "7   0.940432   0.944130   0.353733  0.944444  0.514693  \n",
       "2   0.958822   0.964255   0.460292  0.928105  0.615385  \n",
       "15  0.963886   0.970639   0.522472  0.911765  0.664286  \n",
       "6   0.966285   0.970426   0.497297  0.901961  0.641115  \n",
       "14  0.964152   0.970919   0.512104  0.898693  0.652432  \n",
       "4   0.960954   0.967161   0.461149  0.892157  0.608018  \n",
       "8   0.961087   0.970013   0.478799  0.885621  0.621560  \n",
       "13  0.972148   0.977796   0.555556  0.882353  0.681818  \n",
       "5   0.962420   0.971399   0.500938  0.872549  0.636472  \n",
       "17  0.962820   0.971812   0.482633  0.862745  0.618992  \n",
       "10  0.958955   0.966814   0.454231  0.859477  0.594350  \n",
       "18  0.967351   0.974331   0.512671  0.859477  0.642247  \n",
       "23  0.957489   0.963442   0.483456  0.859477  0.618824  \n",
       "12  0.972148   0.980875   0.587444  0.856209  0.696809  \n",
       "20  0.969216   0.977490   0.551797  0.852941  0.670090  \n",
       "19  0.969883   0.979555   0.579418  0.846405  0.687915  \n",
       "16  0.970282   0.980915   0.582766  0.839869  0.688086  \n",
       "3   0.966018   0.973398   0.520576  0.826797  0.638889  \n",
       "11  0.968150   0.977996   0.534783  0.803922  0.642298  \n",
       "0   0.975880   0.985993   0.624021  0.781046  0.693759  \n",
       "1   0.972681   0.980968   0.565632  0.774510  0.653793  \n",
       "9   0.977745   0.990951   0.695266  0.767974  0.729814  \n",
       "24  0.976146   0.988205   0.638587  0.767974  0.697329  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by=['recall'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\CNN model\\models\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 21s 316us/sample - loss: 0.5980 - acc: 0.8541 - val_loss: 0.4643 - val_acc: 0.8410\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 21s 307us/sample - loss: 0.4390 - acc: 0.8997 - val_loss: 0.3821 - val_acc: 0.8811\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 28s 415us/sample - loss: 0.3857 - acc: 0.9150 - val_loss: 0.3372 - val_acc: 0.9015\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 73s 1ms/sample - loss: 0.3359 - acc: 0.9249 - val_loss: 0.3061 - val_acc: 0.9160\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 71s 1ms/sample - loss: 0.2990 - acc: 0.9340 - val_loss: 0.3742 - val_acc: 0.9014\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.2573 - acc: 0.9422 - val_loss: 0.3177 - val_acc: 0.9395\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 71s 1ms/sample - loss: 0.2379 - acc: 0.9448 - val_loss: 0.3234 - val_acc: 0.9574\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.2183 - acc: 0.9502 - val_loss: 0.3426 - val_acc: 0.9351\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1898 - acc: 0.9551 - val_loss: 0.3262 - val_acc: 0.9574\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 49s 720us/sample - loss: 0.1870 - acc: 0.9576 - val_loss: 0.3286 - val_acc: 0.9419\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 44s 652us/sample - loss: 0.1710 - acc: 0.9615 - val_loss: 0.3223 - val_acc: 0.9568\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 47s 690us/sample - loss: 0.1674 - acc: 0.9611 - val_loss: 0.3771 - val_acc: 0.9689\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 48s 707us/sample - loss: 0.1559 - acc: 0.9635 - val_loss: 0.2912 - val_acc: 0.9406\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 47s 691us/sample - loss: 0.1356 - acc: 0.9682 - val_loss: 0.3189 - val_acc: 0.9574\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 49s 724us/sample - loss: 0.1336 - acc: 0.9698 - val_loss: 0.3222 - val_acc: 0.9418\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 49s 731us/sample - loss: 0.1362 - acc: 0.9691 - val_loss: 0.3690 - val_acc: 0.9626\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 53s 783us/sample - loss: 0.1230 - acc: 0.9718 - val_loss: 0.4578 - val_acc: 0.9689\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 55s 807us/sample - loss: 0.1362 - acc: 0.9697 - val_loss: 0.4612 - val_acc: 0.9671\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 56s 823us/sample - loss: 0.1134 - acc: 0.9737 - val_loss: 0.5298 - val_acc: 0.9654\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 57s 845us/sample - loss: 0.1124 - acc: 0.9751 - val_loss: 0.4833 - val_acc: 0.9398\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 56s 831us/sample - loss: 0.1164 - acc: 0.9742 - val_loss: 0.7403 - val_acc: 0.9736\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 56s 826us/sample - loss: 0.1049 - acc: 0.9758 - val_loss: 0.5902 - val_acc: 0.9632\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 56s 834us/sample - loss: 0.1001 - acc: 0.9773 - val_loss: 0.7072 - val_acc: 0.9606\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 55s 816us/sample - loss: 0.0949 - acc: 0.9783 - val_loss: 0.6343 - val_acc: 0.9638\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 56s 833us/sample - loss: 0.0952 - acc: 0.9783 - val_loss: 0.7345 - val_acc: 0.9733\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 57s 837us/sample - loss: 0.0894 - acc: 0.9798 - val_loss: 0.8150 - val_acc: 0.9659\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.0871 - acc: 0.9814 - val_loss: 0.7709 - val_acc: 0.9741\n",
      "Epoch 31/200\n",
      "67528/67528 [==============================] - 75s 1ms/sample - loss: 0.0974 - acc: 0.9790 - val_loss: 0.6007 - val_acc: 0.9704\n",
      "Epoch 32/200\n",
      "67528/67528 [==============================] - 71s 1ms/sample - loss: 0.0746 - acc: 0.9831 - val_loss: 0.6625 - val_acc: 0.9674\n",
      "Epoch 33/200\n",
      "67528/67528 [==============================] - 72s 1ms/sample - loss: 0.0751 - acc: 0.9831 - val_loss: 1.0169 - val_acc: 0.9759\n",
      "0\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 76s 1ms/sample - loss: 0.5810 - acc: 0.8611 - val_loss: 0.4157 - val_acc: 0.9220\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 59s 875us/sample - loss: 0.4302 - acc: 0.8984 - val_loss: 0.4096 - val_acc: 0.8767\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 45s 666us/sample - loss: 0.3621 - acc: 0.9171 - val_loss: 0.3446 - val_acc: 0.9428\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 45s 672us/sample - loss: 0.3212 - acc: 0.9272 - val_loss: 0.3570 - val_acc: 0.8779\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 47s 699us/sample - loss: 0.2797 - acc: 0.9378 - val_loss: 0.3188 - val_acc: 0.9614\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 47s 695us/sample - loss: 0.2562 - acc: 0.9397 - val_loss: 0.2921 - val_acc: 0.9554\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 49s 725us/sample - loss: 0.2332 - acc: 0.9466 - val_loss: 0.2738 - val_acc: 0.9571\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 51s 758us/sample - loss: 0.2104 - acc: 0.9496 - val_loss: 0.3131 - val_acc: 0.9688\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 52s 765us/sample - loss: 0.1911 - acc: 0.9553 - val_loss: 0.3100 - val_acc: 0.9520\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 55s 814us/sample - loss: 0.1822 - acc: 0.9571 - val_loss: 0.3270 - val_acc: 0.9496\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 57s 843us/sample - loss: 0.1791 - acc: 0.9581 - val_loss: 0.4193 - val_acc: 0.9678\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 57s 841us/sample - loss: 0.1545 - acc: 0.9640 - val_loss: 0.4407 - val_acc: 0.9618\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 58s 852us/sample - loss: 0.1453 - acc: 0.9657 - val_loss: 0.3532 - val_acc: 0.9507\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 51s 761us/sample - loss: 0.1525 - acc: 0.9652 - val_loss: 0.3315 - val_acc: 0.9630\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 43s 633us/sample - loss: 0.1303 - acc: 0.9692 - val_loss: 0.3685 - val_acc: 0.9696\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 57s 845us/sample - loss: 0.1451 - acc: 0.9660 - val_loss: 0.3685 - val_acc: 0.9602\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 57s 846us/sample - loss: 0.1401 - acc: 0.9687 - val_loss: 0.3632 - val_acc: 0.9567\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 55s 815us/sample - loss: 0.1289 - acc: 0.9712 - val_loss: 0.5062 - val_acc: 0.9743\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 58s 856us/sample - loss: 0.1308 - acc: 0.9709 - val_loss: 0.4332 - val_acc: 0.9640\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 56s 825us/sample - loss: 0.1177 - acc: 0.9733 - val_loss: 0.2970 - val_acc: 0.9431\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 56s 834us/sample - loss: 0.1220 - acc: 0.9723 - val_loss: 0.5108 - val_acc: 0.9705\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 56s 829us/sample - loss: 0.1106 - acc: 0.9756 - val_loss: 0.5308 - val_acc: 0.9725\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 56s 824us/sample - loss: 0.1154 - acc: 0.9733 - val_loss: 0.4200 - val_acc: 0.9578\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 56s 825us/sample - loss: 0.1109 - acc: 0.9744 - val_loss: 0.5037 - val_acc: 0.9648\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 57s 837us/sample - loss: 0.1070 - acc: 0.9754 - val_loss: 0.4277 - val_acc: 0.9606\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 55s 808us/sample - loss: 0.1084 - acc: 0.9753 - val_loss: 0.6901 - val_acc: 0.9711\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 56s 830us/sample - loss: 0.0981 - acc: 0.9781 - val_loss: 0.7132 - val_acc: 0.9727\n",
      "1\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 58s 864us/sample - loss: 0.5877 - acc: 0.8614 - val_loss: 0.4027 - val_acc: 0.8598\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 58s 860us/sample - loss: 0.4170 - acc: 0.9050 - val_loss: 0.3475 - val_acc: 0.9142\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 57s 843us/sample - loss: 0.3684 - acc: 0.9137 - val_loss: 0.3752 - val_acc: 0.8934- loss: 0.3678 - acc: 0\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 58s 865us/sample - loss: 0.3228 - acc: 0.9261 - val_loss: 0.2822 - val_acc: 0.9270\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 57s 851us/sample - loss: 0.2826 - acc: 0.9306 - val_loss: 0.3071 - val_acc: 0.9270\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 59s 875us/sample - loss: 0.2645 - acc: 0.9347 - val_loss: 0.3338 - val_acc: 0.9264\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 59s 880us/sample - loss: 0.2340 - acc: 0.9416 - val_loss: 0.2937 - val_acc: 0.9406\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 58s 856us/sample - loss: 0.2183 - acc: 0.9448 - val_loss: 0.2305 - val_acc: 0.9438\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 59s 867us/sample - loss: 0.1968 - acc: 0.9509 - val_loss: 0.2907 - val_acc: 0.9355\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 57s 845us/sample - loss: 0.1851 - acc: 0.9534 - val_loss: 0.3286 - val_acc: 0.9467\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 59s 869us/sample - loss: 0.1783 - acc: 0.9544 - val_loss: 0.3074 - val_acc: 0.9459\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 58s 866us/sample - loss: 0.1603 - acc: 0.9603 - val_loss: 0.3694 - val_acc: 0.9676\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 57s 842us/sample - loss: 0.1632 - acc: 0.9593 - val_loss: 0.3222 - val_acc: 0.9443\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 58s 860us/sample - loss: 0.1583 - acc: 0.9596 - val_loss: 0.4634 - val_acc: 0.9627\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 57s 838us/sample - loss: 0.1405 - acc: 0.9667 - val_loss: 0.3616 - val_acc: 0.8929\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 59s 870us/sample - loss: 0.1370 - acc: 0.9660 - val_loss: 0.4105 - val_acc: 0.9500\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 58s 857us/sample - loss: 0.1428 - acc: 0.9656 - val_loss: 0.3693 - val_acc: 0.9459\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 57s 846us/sample - loss: 0.1381 - acc: 0.9672 - val_loss: 0.3815 - val_acc: 0.9602\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 58s 858us/sample - loss: 0.1482 - acc: 0.9663 - val_loss: 0.3964 - val_acc: 0.9624\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 55s 809us/sample - loss: 0.1389 - acc: 0.9656 - val_loss: 0.5823 - val_acc: 0.9704\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 56s 833us/sample - loss: 0.1245 - acc: 0.9693 - val_loss: 0.5359 - val_acc: 0.9692\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 57s 839us/sample - loss: 0.1293 - acc: 0.9673 - val_loss: 0.3961 - val_acc: 0.9607\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 56s 826us/sample - loss: 0.1373 - acc: 0.9667 - val_loss: 0.4039 - val_acc: 0.9616\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 58s 854us/sample - loss: 0.1325 - acc: 0.9681 - val_loss: 0.3753 - val_acc: 0.9664\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 55s 815us/sample - loss: 0.1272 - acc: 0.9700 - val_loss: 0.5735 - val_acc: 0.9570\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 57s 842us/sample - loss: 0.1186 - acc: 0.9710 - val_loss: 0.3698 - val_acc: 0.9583\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 57s 840us/sample - loss: 0.1238 - acc: 0.9668 - val_loss: 0.5504 - val_acc: 0.9576\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 56s 824us/sample - loss: 0.1148 - acc: 0.9698 - val_loss: 0.3278 - val_acc: 0.9588\n",
      "2\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 59s 869us/sample - loss: 0.5938 - acc: 0.8611 - val_loss: 0.4145 - val_acc: 0.9198\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 56s 824us/sample - loss: 0.4374 - acc: 0.8990 - val_loss: 0.3493 - val_acc: 0.8871\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 57s 846us/sample - loss: 0.3568 - acc: 0.9164 - val_loss: 0.3075 - val_acc: 0.9282\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 56s 828us/sample - loss: 0.3223 - acc: 0.9278 - val_loss: 0.3095 - val_acc: 0.9546\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 57s 850us/sample - loss: 0.3021 - acc: 0.9303 - val_loss: 0.3005 - val_acc: 0.9252\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 57s 845us/sample - loss: 0.2623 - acc: 0.9384 - val_loss: 0.2722 - val_acc: 0.9364\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 56s 829us/sample - loss: 0.2479 - acc: 0.9423 - val_loss: 0.2504 - val_acc: 0.9432\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 57s 845us/sample - loss: 0.2287 - acc: 0.9458 - val_loss: 0.3246 - val_acc: 0.8845\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 52s 776us/sample - loss: 0.2112 - acc: 0.9480 - val_loss: 0.3193 - val_acc: 0.9578\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 43s 630us/sample - loss: 0.2071 - acc: 0.9511 - val_loss: 0.2976 - val_acc: 0.9366\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 57s 843us/sample - loss: 0.1865 - acc: 0.9554 - val_loss: 0.3759 - val_acc: 0.9616\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 57s 850us/sample - loss: 0.1779 - acc: 0.9568 - val_loss: 0.2966 - val_acc: 0.9298\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 56s 822us/sample - loss: 0.1713 - acc: 0.9571 - val_loss: 0.3507 - val_acc: 0.9423\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 57s 840us/sample - loss: 0.1686 - acc: 0.9572 - val_loss: 0.4964 - val_acc: 0.9709\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 55s 820us/sample - loss: 0.1579 - acc: 0.9591 - val_loss: 0.3383 - val_acc: 0.9182\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 57s 847us/sample - loss: 0.1595 - acc: 0.9589 - val_loss: 0.4225 - val_acc: 0.9502\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 57s 843us/sample - loss: 0.1506 - acc: 0.9633 - val_loss: 0.5864 - val_acc: 0.9660\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 55s 817us/sample - loss: 0.1533 - acc: 0.9615 - val_loss: 0.3572 - val_acc: 0.9519\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 57s 850us/sample - loss: 0.1427 - acc: 0.9664 - val_loss: 0.3822 - val_acc: 0.9495\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 57s 837us/sample - loss: 0.1400 - acc: 0.9651 - val_loss: 0.5200 - val_acc: 0.9663\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 57s 848us/sample - loss: 0.1410 - acc: 0.9655 - val_loss: 0.3816 - val_acc: 0.9552\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 58s 855us/sample - loss: 0.1255 - acc: 0.9701 - val_loss: 0.4564 - val_acc: 0.9600\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 57s 840us/sample - loss: 0.1355 - acc: 0.9672 - val_loss: 0.4656 - val_acc: 0.9672\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 58s 854us/sample - loss: 0.1328 - acc: 0.9696 - val_loss: 0.5372 - val_acc: 0.9697\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 57s 840us/sample - loss: 0.1245 - acc: 0.9696 - val_loss: 0.4864 - val_acc: 0.9656\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 57s 846us/sample - loss: 0.1279 - acc: 0.9696 - val_loss: 0.4222 - val_acc: 0.9606\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 57s 847us/sample - loss: 0.1159 - acc: 0.9728 - val_loss: 0.4802 - val_acc: 0.9660\n",
      "3\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 72s 1ms/sample - loss: 0.5939 - acc: 0.8626 - val_loss: 0.3966 - val_acc: 0.8903\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.4352 - acc: 0.9000 - val_loss: 0.3977 - val_acc: 0.9384\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.3787 - acc: 0.9157 - val_loss: 0.2872 - val_acc: 0.9402\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.3265 - acc: 0.9247 - val_loss: 0.2884 - val_acc: 0.9364\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 71s 1ms/sample - loss: 0.3024 - acc: 0.9298 - val_loss: 0.3109 - val_acc: 0.9260\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.2669 - acc: 0.9369 - val_loss: 0.3100 - val_acc: 0.9492\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 71s 1ms/sample - loss: 0.2380 - acc: 0.9429 - val_loss: 0.3251 - val_acc: 0.9494\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.2238 - acc: 0.9481 - val_loss: 0.3141 - val_acc: 0.9627\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 72s 1ms/sample - loss: 0.2270 - acc: 0.9478 - val_loss: 0.2675 - val_acc: 0.9466\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.1950 - acc: 0.9537 - val_loss: 0.2906 - val_acc: 0.9372\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 71s 1ms/sample - loss: 0.1890 - acc: 0.9549 - val_loss: 0.3091 - val_acc: 0.9592\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.1790 - acc: 0.9582 - val_loss: 0.3458 - val_acc: 0.9528\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 72s 1ms/sample - loss: 0.1862 - acc: 0.9572 - val_loss: 0.3685 - val_acc: 0.9484\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 71s 1ms/sample - loss: 0.1665 - acc: 0.9605 - val_loss: 0.3859 - val_acc: 0.9647\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 67s 988us/sample - loss: 0.1472 - acc: 0.9655 - val_loss: 0.3374 - val_acc: 0.9468\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 66s 979us/sample - loss: 0.1521 - acc: 0.9651 - val_loss: 0.4128 - val_acc: 0.9717\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 67s 987us/sample - loss: 0.1453 - acc: 0.9657 - val_loss: 0.4006 - val_acc: 0.9590\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 65s 967us/sample - loss: 0.1493 - acc: 0.9663 - val_loss: 0.3487 - val_acc: 0.9534\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 66s 974us/sample - loss: 0.1315 - acc: 0.9703 - val_loss: 0.4913 - val_acc: 0.9740\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 65s 966us/sample - loss: 0.1382 - acc: 0.9696 - val_loss: 0.3415 - val_acc: 0.9606\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1344 - acc: 0.9685 - val_loss: 0.3317 - val_acc: 0.9402\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 60s 881us/sample - loss: 0.1237 - acc: 0.9706 - val_loss: 0.6630 - val_acc: 0.9741\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 57s 848us/sample - loss: 0.1195 - acc: 0.9724 - val_loss: 0.4489 - val_acc: 0.9640\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 66s 981us/sample - loss: 0.1088 - acc: 0.9741 - val_loss: 0.4440 - val_acc: 0.9676\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 64s 941us/sample - loss: 0.1322 - acc: 0.9696 - val_loss: 0.3777 - val_acc: 0.9663\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 66s 977us/sample - loss: 0.1281 - acc: 0.9720 - val_loss: 0.4256 - val_acc: 0.9672\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 65s 958us/sample - loss: 0.1060 - acc: 0.9761 - val_loss: 0.5010 - val_acc: 0.9683\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 67s 989us/sample - loss: 0.1168 - acc: 0.9734 - val_loss: 0.5101 - val_acc: 0.9709\n",
      "Epoch 29/200\n",
      "67528/67528 [==============================] - 65s 965us/sample - loss: 0.1120 - acc: 0.9742 - val_loss: 0.3670 - val_acc: 0.9610\n",
      "4\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.6084 - acc: 0.8575 - val_loss: 0.6579 - val_acc: 0.9498\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.4541 - acc: 0.8948 - val_loss: 0.3945 - val_acc: 0.9348\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.3973 - acc: 0.9127 - val_loss: 0.3662 - val_acc: 0.9315\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.3529 - acc: 0.9206 - val_loss: 0.3984 - val_acc: 0.8666\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.3107 - acc: 0.9297 - val_loss: 0.3897 - val_acc: 0.9558\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.2812 - acc: 0.9360 - val_loss: 0.2731 - val_acc: 0.9212\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.2720 - acc: 0.9372 - val_loss: 0.2919 - val_acc: 0.9068\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.2343 - acc: 0.9437 - val_loss: 0.2688 - val_acc: 0.9346\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.2368 - acc: 0.9439 - val_loss: 0.3367 - val_acc: 0.9570\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.2150 - acc: 0.9487 - val_loss: 0.2288 - val_acc: 0.9439\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 67s 996us/sample - loss: 0.2074 - acc: 0.9515 - val_loss: 0.2929 - val_acc: 0.9192\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1940 - acc: 0.9564 - val_loss: 0.3178 - val_acc: 0.9543\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 67s 999us/sample - loss: 0.1693 - acc: 0.9593 - val_loss: 0.2758 - val_acc: 0.9436\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1805 - acc: 0.9561 - val_loss: 0.3303 - val_acc: 0.9419\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 67s 996us/sample - loss: 0.1639 - acc: 0.9619 - val_loss: 0.2929 - val_acc: 0.9632\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1547 - acc: 0.9634 - val_loss: 0.3109 - val_acc: 0.9364\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1610 - acc: 0.9620 - val_loss: 0.3023 - val_acc: 0.9514\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1505 - acc: 0.9646 - val_loss: 0.3901 - val_acc: 0.9646\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1311 - acc: 0.9687 - val_loss: 0.4004 - val_acc: 0.9715\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1541 - acc: 0.9648 - val_loss: 0.3926 - val_acc: 0.9664\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 58s 859us/sample - loss: 0.1301 - acc: 0.9696 - val_loss: 0.3514 - val_acc: 0.9492\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 60s 892us/sample - loss: 0.1512 - acc: 0.9647 - val_loss: 0.4457 - val_acc: 0.9606\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 65s 966us/sample - loss: 0.1403 - acc: 0.9690 - val_loss: 0.3221 - val_acc: 0.9667\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 64s 947us/sample - loss: 0.1315 - acc: 0.9699 - val_loss: 0.4512 - val_acc: 0.9697\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 66s 975us/sample - loss: 0.1202 - acc: 0.9722 - val_loss: 0.3309 - val_acc: 0.9462\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 65s 962us/sample - loss: 0.1219 - acc: 0.9718 - val_loss: 0.4754 - val_acc: 0.9678\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 66s 982us/sample - loss: 0.1191 - acc: 0.9735 - val_loss: 0.3753 - val_acc: 0.9683\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 65s 957us/sample - loss: 0.1394 - acc: 0.9689 - val_loss: 0.5392 - val_acc: 0.9697\n",
      "Epoch 29/200\n",
      "67528/67528 [==============================] - 66s 972us/sample - loss: 0.1176 - acc: 0.9726 - val_loss: 0.5473 - val_acc: 0.9733\n",
      "Epoch 30/200\n",
      "67528/67528 [==============================] - 64s 953us/sample - loss: 0.1190 - acc: 0.9725 - val_loss: 0.4573 - val_acc: 0.9624\n",
      "5\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.6002 - acc: 0.8501 - val_loss: 0.4479 - val_acc: 0.9188\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 66s 985us/sample - loss: 0.4567 - acc: 0.8932 - val_loss: 0.3812 - val_acc: 0.9306\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 66s 978us/sample - loss: 0.3927 - acc: 0.9119 - val_loss: 0.5200 - val_acc: 0.8421\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 66s 975us/sample - loss: 0.3469 - acc: 0.9201 - val_loss: 0.3577 - val_acc: 0.9595\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 66s 979us/sample - loss: 0.3148 - acc: 0.9299 - val_loss: 0.2752 - val_acc: 0.9428\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 65s 961us/sample - loss: 0.2921 - acc: 0.9334 - val_loss: 0.2735 - val_acc: 0.9500\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 67s 986us/sample - loss: 0.2712 - acc: 0.9357 - val_loss: 0.3403 - val_acc: 0.9444\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 65s 962us/sample - loss: 0.2532 - acc: 0.9384 - val_loss: 0.2734 - val_acc: 0.9256\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 66s 981us/sample - loss: 0.2286 - acc: 0.9439 - val_loss: 0.2849 - val_acc: 0.9416\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 66s 983us/sample - loss: 0.2229 - acc: 0.9458 - val_loss: 0.2817 - val_acc: 0.9511\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 67s 997us/sample - loss: 0.2069 - acc: 0.9491 - val_loss: 0.2826 - val_acc: 0.9443\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 67s 991us/sample - loss: 0.2041 - acc: 0.9492 - val_loss: 0.2455 - val_acc: 0.9495\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 67s 990us/sample - loss: 0.1957 - acc: 0.9513 - val_loss: 0.3333 - val_acc: 0.9324\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 67s 992us/sample - loss: 0.1833 - acc: 0.9524 - val_loss: 0.3526 - val_acc: 0.9634\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1917 - acc: 0.9540 - val_loss: 0.3202 - val_acc: 0.9638\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 67s 990us/sample - loss: 0.1755 - acc: 0.9578 - val_loss: 0.3375 - val_acc: 0.9600\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1787 - acc: 0.9576 - val_loss: 0.2516 - val_acc: 0.9451\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 66s 982us/sample - loss: 0.1721 - acc: 0.9583 - val_loss: 0.3196 - val_acc: 0.9418\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1651 - acc: 0.9588 - val_loss: 0.4123 - val_acc: 0.9580\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 67s 996us/sample - loss: 0.1591 - acc: 0.9594 - val_loss: 0.3483 - val_acc: 0.9450\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1546 - acc: 0.9609 - val_loss: 0.3097 - val_acc: 0.9462\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 67s 986us/sample - loss: 0.1730 - acc: 0.9585 - val_loss: 0.3981 - val_acc: 0.9688\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 67s 995us/sample - loss: 0.1519 - acc: 0.9618 - val_loss: 0.2843 - val_acc: 0.9599\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 67s 994us/sample - loss: 0.1494 - acc: 0.9633 - val_loss: 0.3229 - val_acc: 0.9419\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1592 - acc: 0.9590 - val_loss: 0.3333 - val_acc: 0.9510\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 67s 987us/sample - loss: 0.1431 - acc: 0.9660 - val_loss: 0.3250 - val_acc: 0.9628\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 67s 997us/sample - loss: 0.1437 - acc: 0.9660 - val_loss: 0.3172 - val_acc: 0.9600\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 67s 993us/sample - loss: 0.1397 - acc: 0.9652 - val_loss: 0.3151 - val_acc: 0.9383\n",
      "Epoch 29/200\n",
      "67528/67528 [==============================] - 67s 993us/sample - loss: 0.1353 - acc: 0.9661 - val_loss: 0.4861 - val_acc: 0.9693\n",
      "Epoch 30/200\n",
      "67528/67528 [==============================] - 67s 997us/sample - loss: 0.1401 - acc: 0.9660 - val_loss: 0.3788 - val_acc: 0.9283\n",
      "Epoch 31/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1378 - acc: 0.9656 - val_loss: 0.3357 - val_acc: 0.9610\n",
      "Epoch 32/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1319 - acc: 0.9693 - val_loss: 0.4185 - val_acc: 0.9663\n",
      "6\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.5874 - acc: 0.8583 - val_loss: 0.6738 - val_acc: 0.7600\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 60s 886us/sample - loss: 0.4542 - acc: 0.8918 - val_loss: 0.3705 - val_acc: 0.8887\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 56s 823us/sample - loss: 0.4012 - acc: 0.9054 - val_loss: 0.3443 - val_acc: 0.9338\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 66s 972us/sample - loss: 0.3689 - acc: 0.9140 - val_loss: 0.3392 - val_acc: 0.9288\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 64s 944us/sample - loss: 0.3368 - acc: 0.9190 - val_loss: 0.3093 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 66s 980us/sample - loss: 0.3076 - acc: 0.9305 - val_loss: 0.2780 - val_acc: 0.9235\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 65s 960us/sample - loss: 0.2716 - acc: 0.9344 - val_loss: 0.2622 - val_acc: 0.9411\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 65s 958us/sample - loss: 0.2590 - acc: 0.9398 - val_loss: 0.3421 - val_acc: 0.9062\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 64s 954us/sample - loss: 0.2358 - acc: 0.9450 - val_loss: 0.2697 - val_acc: 0.9338\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 64s 943us/sample - loss: 0.2421 - acc: 0.9408 - val_loss: 0.2630 - val_acc: 0.9326\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 64s 950us/sample - loss: 0.2117 - acc: 0.9486 - val_loss: 0.2500 - val_acc: 0.9439\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 64s 945us/sample - loss: 0.2090 - acc: 0.9478 - val_loss: 0.2896 - val_acc: 0.9504\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 65s 957us/sample - loss: 0.2027 - acc: 0.9478 - val_loss: 0.3356 - val_acc: 0.9572\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 63s 938us/sample - loss: 0.2066 - acc: 0.9499 - val_loss: 0.3378 - val_acc: 0.9446\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 65s 959us/sample - loss: 0.1931 - acc: 0.9555 - val_loss: 0.3055 - val_acc: 0.9556\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 64s 947us/sample - loss: 0.1813 - acc: 0.9539 - val_loss: 0.3435 - val_acc: 0.9618\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 65s 958us/sample - loss: 0.1839 - acc: 0.9559 - val_loss: 0.3557 - val_acc: 0.9542\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 64s 948us/sample - loss: 0.1732 - acc: 0.9567 - val_loss: 0.2961 - val_acc: 0.9608\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 65s 956us/sample - loss: 0.1798 - acc: 0.9567 - val_loss: 0.2970 - val_acc: 0.9475\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 65s 958us/sample - loss: 0.1625 - acc: 0.9572 - val_loss: 0.3886 - val_acc: 0.9586\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 64s 949us/sample - loss: 0.1676 - acc: 0.9588 - val_loss: 0.3244 - val_acc: 0.9523\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 65s 960us/sample - loss: 0.1559 - acc: 0.9621 - val_loss: 0.3314 - val_acc: 0.9426\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 64s 944us/sample - loss: 0.1554 - acc: 0.9612 - val_loss: 0.4221 - val_acc: 0.9520\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 65s 958us/sample - loss: 0.1535 - acc: 0.9610 - val_loss: 0.4738 - val_acc: 0.9591\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 64s 952us/sample - loss: 0.1473 - acc: 0.9619 - val_loss: 0.5271 - val_acc: 0.9611\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 64s 953us/sample - loss: 0.1522 - acc: 0.9626 - val_loss: 0.4114 - val_acc: 0.9550\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 64s 945us/sample - loss: 0.1495 - acc: 0.9624 - val_loss: 0.2856 - val_acc: 0.9499\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 55s 821us/sample - loss: 0.1544 - acc: 0.9608 - val_loss: 0.3197 - val_acc: 0.9436\n",
      "Epoch 29/200\n",
      "67528/67528 [==============================] - 60s 884us/sample - loss: 0.1504 - acc: 0.9632 - val_loss: 0.5236 - val_acc: 0.9745\n",
      "Epoch 30/200\n",
      "67528/67528 [==============================] - 66s 977us/sample - loss: 0.1501 - acc: 0.9652 - val_loss: 0.3997 - val_acc: 0.9646\n",
      "Epoch 31/200\n",
      "67528/67528 [==============================] - 65s 964us/sample - loss: 0.1383 - acc: 0.9675 - val_loss: 0.3366 - val_acc: 0.9404\n",
      "7\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 61s 898us/sample - loss: 0.5890 - acc: 0.8599 - val_loss: 0.4105 - val_acc: 0.8949\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 58s 863us/sample - loss: 0.4326 - acc: 0.8950 - val_loss: 0.3562 - val_acc: 0.9471\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 59s 878us/sample - loss: 0.3832 - acc: 0.9125 - val_loss: 0.3178 - val_acc: 0.9163\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 58s 861us/sample - loss: 0.3312 - acc: 0.9222 - val_loss: 0.2946 - val_acc: 0.9402\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 59s 872us/sample - loss: 0.2930 - acc: 0.9305 - val_loss: 0.2932 - val_acc: 0.9124\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 58s 859us/sample - loss: 0.2642 - acc: 0.9384 - val_loss: 0.2961 - val_acc: 0.9662\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 59s 870us/sample - loss: 0.2264 - acc: 0.9439 - val_loss: 0.3180 - val_acc: 0.9564\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 58s 865us/sample - loss: 0.2032 - acc: 0.9528 - val_loss: 0.2684 - val_acc: 0.9378\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 58s 853us/sample - loss: 0.1871 - acc: 0.9541 - val_loss: 0.3013 - val_acc: 0.9620\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 58s 863us/sample - loss: 0.1839 - acc: 0.9547 - val_loss: 0.2636 - val_acc: 0.9515\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 57s 846us/sample - loss: 0.1881 - acc: 0.9565 - val_loss: 0.2668 - val_acc: 0.9640\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 59s 871us/sample - loss: 0.1529 - acc: 0.9626 - val_loss: 0.4398 - val_acc: 0.9610\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 59s 874us/sample - loss: 0.1542 - acc: 0.9636 - val_loss: 0.3594 - val_acc: 0.9630\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 58s 859us/sample - loss: 0.1494 - acc: 0.9645 - val_loss: 0.3049 - val_acc: 0.9619\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 58s 862us/sample - loss: 0.1380 - acc: 0.9681 - val_loss: 0.3470 - val_acc: 0.9491\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 58s 856us/sample - loss: 0.1331 - acc: 0.9684 - val_loss: 0.3703 - val_acc: 0.9697\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 59s 869us/sample - loss: 0.1371 - acc: 0.9680 - val_loss: 0.2941 - val_acc: 0.9514\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 58s 864us/sample - loss: 0.1286 - acc: 0.9698 - val_loss: 0.3634 - val_acc: 0.9587\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 58s 857us/sample - loss: 0.1217 - acc: 0.9721 - val_loss: 0.3808 - val_acc: 0.9655\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 59s 874us/sample - loss: 0.1132 - acc: 0.9736 - val_loss: 0.4330 - val_acc: 0.9656\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 57s 850us/sample - loss: 0.1298 - acc: 0.9716 - val_loss: 0.3974 - val_acc: 0.9674\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 58s 857us/sample - loss: 0.1188 - acc: 0.9737 - val_loss: 0.4011 - val_acc: 0.9534\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 58s 857us/sample - loss: 0.1140 - acc: 0.9736 - val_loss: 0.5898 - val_acc: 0.9700\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 58s 856us/sample - loss: 0.1186 - acc: 0.9729 - val_loss: 0.4374 - val_acc: 0.9699\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 58s 861us/sample - loss: 0.1030 - acc: 0.9763 - val_loss: 0.4535 - val_acc: 0.9623\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 58s 854us/sample - loss: 0.1010 - acc: 0.9766 - val_loss: 0.6330 - val_acc: 0.9744\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 59s 873us/sample - loss: 0.1133 - acc: 0.9756 - val_loss: 0.4391 - val_acc: 0.9674\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 59s 868us/sample - loss: 0.0999 - acc: 0.9772 - val_loss: 0.7873 - val_acc: 0.9658\n",
      "Epoch 29/200\n",
      "67528/67528 [==============================] - 51s 761us/sample - loss: 0.1053 - acc: 0.9773 - val_loss: 0.6001 - val_acc: 0.9732\n",
      "Epoch 30/200\n",
      "67528/67528 [==============================] - 46s 682us/sample - loss: 0.0896 - acc: 0.9809 - val_loss: 0.3718 - val_acc: 0.9611\n",
      "8\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 60s 891us/sample - loss: 0.5901 - acc: 0.8582 - val_loss: 0.5163 - val_acc: 0.9332\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 56s 824us/sample - loss: 0.4413 - acc: 0.8990 - val_loss: 0.3491 - val_acc: 0.9123\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 57s 850us/sample - loss: 0.3756 - acc: 0.9164 - val_loss: 0.3159 - val_acc: 0.9390\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 57s 844us/sample - loss: 0.3218 - acc: 0.9277 - val_loss: 0.3466 - val_acc: 0.9378\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 56s 830us/sample - loss: 0.2829 - acc: 0.9359 - val_loss: 0.4523 - val_acc: 0.8571\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 58s 854us/sample - loss: 0.2487 - acc: 0.9432 - val_loss: 0.3021 - val_acc: 0.9216\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 56s 827us/sample - loss: 0.2252 - acc: 0.9486 - val_loss: 0.3330 - val_acc: 0.9522\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 57s 838us/sample - loss: 0.1995 - acc: 0.9541 - val_loss: 0.3295 - val_acc: 0.9654\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 57s 839us/sample - loss: 0.1743 - acc: 0.9602 - val_loss: 0.3215 - val_acc: 0.9483\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 56s 822us/sample - loss: 0.1657 - acc: 0.9623 - val_loss: 0.2928 - val_acc: 0.9495\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 56s 837us/sample - loss: 0.1432 - acc: 0.9663 - val_loss: 0.3570 - val_acc: 0.9674\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 57s 844us/sample - loss: 0.1398 - acc: 0.9683 - val_loss: 0.3360 - val_acc: 0.9422\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 56s 832us/sample - loss: 0.1522 - acc: 0.9659 - val_loss: 0.3296 - val_acc: 0.9615\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 57s 845us/sample - loss: 0.1374 - acc: 0.9694 - val_loss: 0.3454 - val_acc: 0.9563\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 56s 833us/sample - loss: 0.1270 - acc: 0.9707 - val_loss: 0.3732 - val_acc: 0.9656\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 57s 839us/sample - loss: 0.1172 - acc: 0.9742 - val_loss: 0.4308 - val_acc: 0.9624\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 57s 843us/sample - loss: 0.1224 - acc: 0.9729 - val_loss: 0.3527 - val_acc: 0.9650\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 56s 833us/sample - loss: 0.0993 - acc: 0.9776 - val_loss: 0.3754 - val_acc: 0.9686\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 57s 841us/sample - loss: 0.1054 - acc: 0.9775 - val_loss: 0.5426 - val_acc: 0.9744\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 57s 840us/sample - loss: 0.1034 - acc: 0.9768 - val_loss: 0.5237 - val_acc: 0.9739\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 56s 827us/sample - loss: 0.0910 - acc: 0.9794 - val_loss: 0.5806 - val_acc: 0.9743\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 57s 846us/sample - loss: 0.0903 - acc: 0.9802 - val_loss: 0.6019 - val_acc: 0.9715\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 55s 822us/sample - loss: 0.1010 - acc: 0.9786 - val_loss: 0.5561 - val_acc: 0.9687\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 56s 836us/sample - loss: 0.1029 - acc: 0.9778 - val_loss: 0.5364 - val_acc: 0.9727\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 56s 829us/sample - loss: 0.0712 - acc: 0.9840 - val_loss: 0.5414 - val_acc: 0.9756\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 56s 825us/sample - loss: 0.0891 - acc: 0.9812 - val_loss: 0.5120 - val_acc: 0.9761\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 57s 837us/sample - loss: 0.0743 - acc: 0.9838 - val_loss: 0.6103 - val_acc: 0.9699\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 56s 831us/sample - loss: 0.0887 - acc: 0.9818 - val_loss: 0.5608 - val_acc: 0.9753\n",
      "Epoch 29/200\n",
      "67528/67528 [==============================] - 56s 825us/sample - loss: 0.0802 - acc: 0.9833 - val_loss: 0.4246 - val_acc: 0.9586\n",
      "Epoch 30/200\n",
      "67528/67528 [==============================] - 56s 835us/sample - loss: 0.0680 - acc: 0.9845 - val_loss: 0.7836 - val_acc: 0.9777\n",
      "9\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 60s 891us/sample - loss: 0.5911 - acc: 0.8596 - val_loss: 0.4047 - val_acc: 0.8899\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 57s 843us/sample - loss: 0.4549 - acc: 0.8939 - val_loss: 0.3907 - val_acc: 0.8810\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 57s 843us/sample - loss: 0.3929 - acc: 0.9087 - val_loss: 0.3695 - val_acc: 0.8767\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 57s 848us/sample - loss: 0.3286 - acc: 0.9210 - val_loss: 0.3728 - val_acc: 0.8903\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 57s 851us/sample - loss: 0.3051 - acc: 0.9256 - val_loss: 0.3248 - val_acc: 0.8847\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 57s 840us/sample - loss: 0.2584 - acc: 0.9363 - val_loss: 0.3399 - val_acc: 0.9523\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 58s 856us/sample - loss: 0.2511 - acc: 0.9393 - val_loss: 0.3111 - val_acc: 0.9384\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 57s 847us/sample - loss: 0.2285 - acc: 0.9444 - val_loss: 0.3160 - val_acc: 0.8977\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 59s 871us/sample - loss: 0.2133 - acc: 0.9506 - val_loss: 0.3005 - val_acc: 0.9491\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 59s 869us/sample - loss: 0.1945 - acc: 0.9525 - val_loss: 0.3548 - val_acc: 0.9415\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 57s 846us/sample - loss: 0.2033 - acc: 0.9517 - val_loss: 0.3699 - val_acc: 0.9672\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 58s 856us/sample - loss: 0.1691 - acc: 0.9591 - val_loss: 0.2901 - val_acc: 0.9326\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 58s 854us/sample - loss: 0.1701 - acc: 0.9579 - val_loss: 0.4060 - val_acc: 0.9658\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 58s 858us/sample - loss: 0.1648 - acc: 0.9590 - val_loss: 0.4403 - val_acc: 0.9628\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 58s 858us/sample - loss: 0.1522 - acc: 0.9630 - val_loss: 0.3778 - val_acc: 0.9595\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 57s 844us/sample - loss: 0.1602 - acc: 0.9644 - val_loss: 0.3005 - val_acc: 0.9472\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 58s 852us/sample - loss: 0.1390 - acc: 0.9657 - val_loss: 0.6325 - val_acc: 0.9678\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 58s 859us/sample - loss: 0.1439 - acc: 0.9643 - val_loss: 0.3845 - val_acc: 0.9704\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 51s 750us/sample - loss: 0.1593 - acc: 0.9660 - val_loss: 0.3506 - val_acc: 0.9618\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 50s 740us/sample - loss: 0.1256 - acc: 0.9702 - val_loss: 0.4726 - val_acc: 0.9700\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 61s 910us/sample - loss: 0.1174 - acc: 0.9726 - val_loss: 0.6226 - val_acc: 0.9675\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 59s 872us/sample - loss: 0.1264 - acc: 0.9716 - val_loss: 0.3452 - val_acc: 0.9519\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 60s 892us/sample - loss: 0.1229 - acc: 0.9704 - val_loss: 0.6748 - val_acc: 0.9739\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 59s 879us/sample - loss: 0.1210 - acc: 0.9718 - val_loss: 0.5557 - val_acc: 0.9682\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 59s 879us/sample - loss: 0.1079 - acc: 0.9753 - val_loss: 0.7078 - val_acc: 0.9743\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 60s 882us/sample - loss: 0.1169 - acc: 0.9726 - val_loss: 0.5756 - val_acc: 0.9704\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 59s 880us/sample - loss: 0.1214 - acc: 0.9724 - val_loss: 0.4459 - val_acc: 0.9583\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 60s 889us/sample - loss: 0.1163 - acc: 0.9740 - val_loss: 0.5634 - val_acc: 0.9631\n",
      "Epoch 29/200\n",
      "67528/67528 [==============================] - 60s 895us/sample - loss: 0.0994 - acc: 0.9769 - val_loss: 0.6734 - val_acc: 0.9740\n",
      "Epoch 30/200\n",
      "67528/67528 [==============================] - 59s 877us/sample - loss: 0.1111 - acc: 0.9746 - val_loss: 0.5701 - val_acc: 0.9695\n",
      "Epoch 31/200\n",
      "67528/67528 [==============================] - 60s 890us/sample - loss: 0.0981 - acc: 0.9773 - val_loss: 0.6054 - val_acc: 0.9739\n",
      "Epoch 32/200\n",
      "67528/67528 [==============================] - 59s 876us/sample - loss: 0.1075 - acc: 0.9754 - val_loss: 0.4602 - val_acc: 0.9590\n",
      "10\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 63s 934us/sample - loss: 0.5899 - acc: 0.8583 - val_loss: 0.4641 - val_acc: 0.9367\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 59s 874us/sample - loss: 0.4400 - acc: 0.8998 - val_loss: 0.3570 - val_acc: 0.9312\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 60s 891us/sample - loss: 0.3858 - acc: 0.9113 - val_loss: 0.4263 - val_acc: 0.8509\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 59s 878us/sample - loss: 0.3354 - acc: 0.9222 - val_loss: 0.3104 - val_acc: 0.9068\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 60s 884us/sample - loss: 0.3047 - acc: 0.9296 - val_loss: 0.2913 - val_acc: 0.9388\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 59s 874us/sample - loss: 0.2745 - acc: 0.9332 - val_loss: 0.3273 - val_acc: 0.9476\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 60s 893us/sample - loss: 0.2423 - acc: 0.9413 - val_loss: 0.3046 - val_acc: 0.9436\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 59s 880us/sample - loss: 0.2193 - acc: 0.9444 - val_loss: 0.2727 - val_acc: 0.9430\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 59s 875us/sample - loss: 0.2113 - acc: 0.9488 - val_loss: 0.2838 - val_acc: 0.9332\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 60s 889us/sample - loss: 0.1873 - acc: 0.9522 - val_loss: 0.3807 - val_acc: 0.9655\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 59s 873us/sample - loss: 0.1926 - acc: 0.9521 - val_loss: 0.2885 - val_acc: 0.9543\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 60s 885us/sample - loss: 0.1646 - acc: 0.9591 - val_loss: 0.4474 - val_acc: 0.8870\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 60s 891us/sample - loss: 0.1729 - acc: 0.9551 - val_loss: 0.2989 - val_acc: 0.9408\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 59s 874us/sample - loss: 0.1558 - acc: 0.9597 - val_loss: 0.3772 - val_acc: 0.9510\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 60s 884us/sample - loss: 0.1601 - acc: 0.9595 - val_loss: 0.3193 - val_acc: 0.9391\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 59s 874us/sample - loss: 0.1527 - acc: 0.9614 - val_loss: 0.3333 - val_acc: 0.9592\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 60s 887us/sample - loss: 0.1451 - acc: 0.9631 - val_loss: 0.5005 - val_acc: 0.9731\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 60s 883us/sample - loss: 0.1399 - acc: 0.9668 - val_loss: 0.3496 - val_acc: 0.9539\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 59s 879us/sample - loss: 0.1752 - acc: 0.9617 - val_loss: 0.3397 - val_acc: 0.9336\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 59s 873us/sample - loss: 0.1294 - acc: 0.9704 - val_loss: 0.3005 - val_acc: 0.9579\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 59s 870us/sample - loss: 0.1470 - acc: 0.9675 - val_loss: 0.3804 - val_acc: 0.9648\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 59s 880us/sample - loss: 0.1121 - acc: 0.9742 - val_loss: 0.5096 - val_acc: 0.9693\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 59s 868us/sample - loss: 0.1201 - acc: 0.9724 - val_loss: 0.4171 - val_acc: 0.9326\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 59s 881us/sample - loss: 0.1243 - acc: 0.9707 - val_loss: 0.5992 - val_acc: 0.9708\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 61s 897us/sample - loss: 0.1183 - acc: 0.9698 - val_loss: 0.4648 - val_acc: 0.9668\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 52s 764us/sample - loss: 0.1202 - acc: 0.9719 - val_loss: 0.3533 - val_acc: 0.9411\n",
      "Epoch 27/200\n",
      "67528/67528 [==============================] - 46s 677us/sample - loss: 0.1254 - acc: 0.9683 - val_loss: 0.4698 - val_acc: 0.9626\n",
      "Epoch 28/200\n",
      "67528/67528 [==============================] - 61s 898us/sample - loss: 0.1172 - acc: 0.9723 - val_loss: 0.5333 - val_acc: 0.9682\n",
      "11\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.6057 - acc: 0.8582 - val_loss: 0.4521 - val_acc: 0.8717\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.4612 - acc: 0.8932 - val_loss: 0.3265 - val_acc: 0.9158\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.3806 - acc: 0.9132 - val_loss: 0.3743 - val_acc: 0.8793\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.3316 - acc: 0.9259 - val_loss: 0.3033 - val_acc: 0.9132\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 67s 996us/sample - loss: 0.2939 - acc: 0.9332 - val_loss: 0.2898 - val_acc: 0.9476\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.2680 - acc: 0.9389 - val_loss: 0.2387 - val_acc: 0.9360\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 67s 999us/sample - loss: 0.2378 - acc: 0.9445 - val_loss: 0.3364 - val_acc: 0.9479\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.2157 - acc: 0.9495 - val_loss: 0.2610 - val_acc: 0.9556\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.2091 - acc: 0.9515 - val_loss: 0.3068 - val_acc: 0.9579\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1889 - acc: 0.9560 - val_loss: 0.3324 - val_acc: 0.9644\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 68s 1000us/sample - loss: 0.1784 - acc: 0.9598 - val_loss: 0.2808 - val_acc: 0.9450\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1784 - acc: 0.9593 - val_loss: 0.3656 - val_acc: 0.9648\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1609 - acc: 0.9636 - val_loss: 0.2962 - val_acc: 0.9352\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1608 - acc: 0.9635 - val_loss: 0.3033 - val_acc: 0.9616\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 67s 999us/sample - loss: 0.1380 - acc: 0.9683 - val_loss: 0.3344 - val_acc: 0.9574\n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1434 - acc: 0.9689 - val_loss: 0.4163 - val_acc: 0.9713\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.1296 - acc: 0.9710 - val_loss: 0.4132 - val_acc: 0.9675\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1363 - acc: 0.9704 - val_loss: 0.4312 - val_acc: 0.9611\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1321 - acc: 0.9717 - val_loss: 0.4148 - val_acc: 0.9680\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 67s 994us/sample - loss: 0.1215 - acc: 0.9727 - val_loss: 0.3391 - val_acc: 0.9514\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 68s 1ms/sample - loss: 0.1313 - acc: 0.9714 - val_loss: 0.5203 - val_acc: 0.9716\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 59s 878us/sample - loss: 0.1216 - acc: 0.9739 - val_loss: 0.4012 - val_acc: 0.9452\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 56s 823us/sample - loss: 0.1104 - acc: 0.9761 - val_loss: 0.3634 - val_acc: 0.9699\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 67s 994us/sample - loss: 0.1073 - acc: 0.9763 - val_loss: 0.4906 - val_acc: 0.9713\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 65s 968us/sample - loss: 0.1052 - acc: 0.9769 - val_loss: 0.3674 - val_acc: 0.9652\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 68s 1000us/sample - loss: 0.1041 - acc: 0.9768 - val_loss: 0.5358 - val_acc: 0.9721\n",
      "12\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 70s 1ms/sample - loss: 0.6330 - acc: 0.8468 - val_loss: 0.3954 - val_acc: 0.8841\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.4579 - acc: 0.8996 - val_loss: 0.3756 - val_acc: 0.8839\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 67s 995us/sample - loss: 0.3910 - acc: 0.9142 - val_loss: 0.4498 - val_acc: 0.8397\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 69s 1ms/sample - loss: 0.3433 - acc: 0.9240 - val_loss: 0.3218 - val_acc: 0.9388\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 67s 994us/sample - loss: 0.3077 - acc: 0.9321 - val_loss: 0.2801 - val_acc: 0.9280\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 92s 1ms/sample - loss: 0.1389 - acc: 0.9706 - val_loss: 0.5384 - val_acc: 0.9699\n",
      "19\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 112s 2ms/sample - loss: 0.6114 - acc: 0.8580 - val_loss: 0.4234 - val_acc: 0.8793\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.4907 - acc: 0.8859 - val_loss: 0.4251 - val_acc: 0.8754\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.4194 - acc: 0.9047 - val_loss: 0.3925 - val_acc: 0.9298\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.3712 - acc: 0.9164 - val_loss: 0.4788 - val_acc: 0.8182\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.3473 - acc: 0.9225 - val_loss: 0.3004 - val_acc: 0.9336\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.2986 - acc: 0.9313 - val_loss: 0.2706 - val_acc: 0.9300\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 105s 2ms/sample - loss: 0.3249 - acc: 0.9302 - val_loss: 0.2651 - val_acc: 0.9392\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.2730 - acc: 0.9374 - val_loss: 0.3200 - val_acc: 0.9519\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 105s 2ms/sample - loss: 0.2583 - acc: 0.9386 - val_loss: 0.3160 - val_acc: 0.9402\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 105s 2ms/sample - loss: 0.2420 - acc: 0.9438 - val_loss: 0.3442 - val_acc: 0.9411\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.2217 - acc: 0.9476 - val_loss: 0.3124 - val_acc: 0.9148\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.2123 - acc: 0.9492 - val_loss: 0.3119 - val_acc: 0.9400\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 105s 2ms/sample - loss: 0.2132 - acc: 0.9490 - val_loss: 0.3306 - val_acc: 0.9515\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 105s 2ms/sample - loss: 0.1328 - acc: 0.9710 - val_loss: 0.4244 - val_acc: 0.9604\n",
      "Epoch 36/200\n",
      "67528/67528 [==============================] - 103s 2ms/sample - loss: 0.1381 - acc: 0.9690 - val_loss: 0.4564 - val_acc: 0.9686\n",
      "Epoch 37/200\n",
      "67528/67528 [==============================] - 105s 2ms/sample - loss: 0.1343 - acc: 0.9707 - val_loss: 0.5097 - val_acc: 0.9636\n",
      "Epoch 38/200\n",
      "67528/67528 [==============================] - 105s 2ms/sample - loss: 0.1325 - acc: 0.9709 - val_loss: 0.5785 - val_acc: 0.9656\n",
      "Epoch 39/200\n",
      "67528/67528 [==============================] - 97s 1ms/sample - loss: 0.1226 - acc: 0.9722 - val_loss: 0.5188 - val_acc: 0.9692\n",
      "20\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 110s 2ms/sample - loss: 0.6378 - acc: 0.8482 - val_loss: 0.5121 - val_acc: 0.9114\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.4980 - acc: 0.8825 - val_loss: 0.4615 - val_acc: 0.9276\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.4565 - acc: 0.8964 - val_loss: 0.3669 - val_acc: 0.9298\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.4058 - acc: 0.9106 - val_loss: 0.3748 - val_acc: 0.9278\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.3700 - acc: 0.9200 - val_loss: 0.3356 - val_acc: 0.9474\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.3515 - acc: 0.9245 - val_loss: 0.3609 - val_acc: 0.9038\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.3217 - acc: 0.9309 - val_loss: 0.3296 - val_acc: 0.9279\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.3028 - acc: 0.9355 - val_loss: 0.2898 - val_acc: 0.9347\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.2783 - acc: 0.9373 - val_loss: 0.2863 - val_acc: 0.9336\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.2730 - acc: 0.9375 - val_loss: 0.4179 - val_acc: 0.9490\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.2580 - acc: 0.9425 - val_loss: 0.2911 - val_acc: 0.9386\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.2840 - acc: 0.9414 - val_loss: 0.3427 - val_acc: 0.9570\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.2319 - acc: 0.9473 - val_loss: 0.2880 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "34016/67528 [==============>...............] - ETA: 49s - loss: 0.2104 - acc: 0.9506 ETA: 50s - loss: 0.2091 - a21\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 114s 2ms/sample - loss: 0.6241 - acc: 0.8509 - val_loss: 0.7375 - val_acc: 0.9492\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.4679 - acc: 0.8891 - val_loss: 0.3284 - val_acc: 0.9224\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 109s 2ms/sample - loss: 0.4096 - acc: 0.9103 - val_loss: 0.3365 - val_acc: 0.8979\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.3602 - acc: 0.9181 - val_loss: 0.2832 - val_acc: 0.9168\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.3522 - acc: 0.9207 - val_loss: 0.2827 - val_acc: 0.9142\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.3067 - acc: 0.9281 - val_loss: 0.2291 - val_acc: 0.9464\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.3064 - acc: 0.9310 - val_loss: 0.3914 - val_acc: 0.9322\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 109s 2ms/sample - loss: 0.2761 - acc: 0.9349 - val_loss: 0.2736 - val_acc: 0.9307\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2586 - acc: 0.9388 - val_loss: 0.3213 - val_acc: 0.96728\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 106s 2ms/sample - loss: 0.2672 - acc: 0.9369 - val_loss: 0.3183 - val_acc: 0.9592\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2343 - acc: 0.9413 - val_loss: 0.2852 - val_acc: 0.9314\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2283 - acc: 0.9410 - val_loss: 0.2855 - val_acc: 0.9492\n",
      "Epoch 13/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2172 - acc: 0.9467 - val_loss: 0.2451 - val_acc: 0.9415\n",
      "Epoch 14/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2146 - acc: 0.9456 - val_loss: 0.3261 - val_acc: 0.9624\n",
      "Epoch 15/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2180 - acc: 0.9473 - val_loss: 0.3304 - val_acc: 0.9623c: \n",
      "Epoch 16/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2025 - acc: 0.9524 - val_loss: 0.3515 - val_acc: 0.9623\n",
      "Epoch 17/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2048 - acc: 0.9513 - val_loss: 0.3460 - val_acc: 0.9602\n",
      "Epoch 18/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.1939 - acc: 0.9560 - val_loss: 0.4154 - val_acc: 0.9504\n",
      "Epoch 19/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.2123 - acc: 0.9484 - val_loss: 0.2557 - val_acc: 0.9380\n",
      "Epoch 20/200\n",
      "67528/67528 [==============================] - 107s 2ms/sample - loss: 0.1765 - acc: 0.9569 - val_loss: 0.3121 - val_acc: 0.9516\n",
      "Epoch 21/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.1893 - acc: 0.9548 - val_loss: 0.3770 - val_acc: 0.8981\n",
      "Epoch 22/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.1916 - acc: 0.9507 - val_loss: 0.3430 - val_acc: 0.9244\n",
      "Epoch 23/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.1838 - acc: 0.9542 - val_loss: 0.3263 - val_acc: 0.9199\n",
      "Epoch 24/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.1858 - acc: 0.9532 - val_loss: 0.5744 - val_acc: 0.9655\n",
      "Epoch 25/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.1722 - acc: 0.9587 - val_loss: 0.3719 - val_acc: 0.9515\n",
      "Epoch 26/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.1974 - acc: 0.9499 - val_loss: 0.2914 - val_acc: 0.9408\n",
      "22\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 116s 2ms/sample - loss: 0.6730 - acc: 0.8420 - val_loss: 0.5305 - val_acc: 0.9066\n",
      "Epoch 2/200\n",
      "67528/67528 [==============================] - 111s 2ms/sample - loss: 0.5295 - acc: 0.8767 - val_loss: 0.4547 - val_acc: 0.8511\n",
      "Epoch 3/200\n",
      "67528/67528 [==============================] - 110s 2ms/sample - loss: 0.4663 - acc: 0.8898 - val_loss: 0.4106 - val_acc: 0.9104\n",
      "Epoch 4/200\n",
      "67528/67528 [==============================] - 111s 2ms/sample - loss: 0.4275 - acc: 0.9012 - val_loss: 0.3642 - val_acc: 0.9294\n",
      "Epoch 5/200\n",
      "67528/67528 [==============================] - 112s 2ms/sample - loss: 0.3923 - acc: 0.9092 - val_loss: 0.4014 - val_acc: 0.9619\n",
      "Epoch 6/200\n",
      "67528/67528 [==============================] - 101s 1ms/sample - loss: 0.3902 - acc: 0.9130 - val_loss: 0.2961 - val_acc: 0.9386\n",
      "Epoch 7/200\n",
      "67528/67528 [==============================] - 91s 1ms/sample - loss: 0.3510 - acc: 0.9203 - val_loss: 0.3141 - val_acc: 0.92511s - loss\n",
      "Epoch 8/200\n",
      "67528/67528 [==============================] - 111s 2ms/sample - loss: 0.3164 - acc: 0.9268 - val_loss: 0.3618 - val_acc: 0.8858\n",
      "Epoch 9/200\n",
      "67528/67528 [==============================] - 110s 2ms/sample - loss: 0.3055 - acc: 0.9266 - val_loss: 0.3141 - val_acc: 0.9403\n",
      "Epoch 10/200\n",
      "67528/67528 [==============================] - 108s 2ms/sample - loss: 0.2914 - acc: 0.9293 - val_loss: 0.3334 - val_acc: 0.9166\n",
      "Epoch 11/200\n",
      "67528/67528 [==============================] - 111s 2ms/sample - loss: 0.2706 - acc: 0.9345 - val_loss: 0.2972 - val_acc: 0.9298\n",
      "Epoch 12/200\n",
      "67528/67528 [==============================] - 111s 2ms/sample - loss: 0.2605 - acc: 0.9356 - val_loss: 0.3364 - val_acc: 0.9198\n",
      "Epoch 13/200\n",
      "54464/67528 [=======================>......] - ETA: 21s - loss: 0.0879 - acc: 0.981024\n",
      "Train on 67528 samples, validate on 7504 samples\n",
      "Epoch 1/200\n",
      "67528/67528 [==============================] - 111s 2ms/sample - loss: 0.1688 - acc: 0.9613 - val_loss: 0.3823 - val_acc: 0.8953\n",
      "Epoch 15/200\n",
      "44896/67528 [==================>...........] - ETA: 38s - loss: 0.1381 - acc: 0.9663"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-920478861e34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_intial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                     \u001b[0mpred_y\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\eric_\\OneDrive\\Documents\\python workbooks\\FreshAir\\CNN model\\models\"\n",
    "results_df = pd.DataFrame()\n",
    "class_weight = {0: 1, 1: 30}\n",
    "#BatchSizes  = [32]\n",
    "NumberOFilters = [64,128]\n",
    "NumberOfNodes = [64,128]\n",
    "FilterLayers = [2,3]\n",
    "NormLayers = [2,3]\n",
    "WeightInitialization = ['he_uniform', 'he_normal']\n",
    "i = 0\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=.0001, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "for num_filters in NumberOFilters:\n",
    "    for num_nodes in NumberOfNodes:\n",
    "        for num_layers_filter in FilterLayers:\n",
    "            for num_layers_norm in NormLayers:\n",
    "                for weight_intial in WeightInitialization:\n",
    "                    model = Sequential()\n",
    "                    model.add(Conv1D(filters=num_filters ,kernel_size = 2, activation='relu', kernel_initializer = weight_intial ,input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "                    model.add(MaxPooling1D(pool_size=2))\n",
    "                    model.add(Conv1D(filters=num_filters,kernel_initializer = weight_intial,kernel_size = 2 ,activation='relu'))\n",
    "                    model.add(MaxPooling1D(pool_size=2))\n",
    "                    if num_layers_filter == 3:\n",
    "                        model.add(Conv1D(filters=num_filters,kernel_initializer = weight_intial,kernel_size = 2 ,activation='relu'))\n",
    "                        model.add(MaxPooling1D(pool_size=2))\n",
    "                    model.add(Flatten())\n",
    "                    model.add(Dense(num_nodes,kernel_initializer = weight_intial, activation='elu'))\n",
    "                    model.add(Dense(num_nodes/2,kernel_initializer =weight_intial, activation= 'elu'))\n",
    "                    if num_layers_norm == 3:\n",
    "                        model.add(Dense(num_nodes/4,kernel_initializer =weight_intial, activation= 'elu'))\n",
    "                    model.add(Dense(1, kernel_initializer = weight_intial, activation='sigmoid'))\n",
    "                    model.compile(optimizer='adam',loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "                    history = model.fit(x_train, y_train, batch_size=32, epochs=200, use_multiprocessing=True,validation_split=0.1, callbacks = [early_stop],class_weight = class_weight)\n",
    "                    pred_y  = model.predict_classes(x_test)\n",
    "                    train_loss, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
    "                    historydf = pd.DataFrame.from_dict(history.history)\n",
    "                    recall_score = sklearn.metrics.recall_score(y_test,pred_y)\n",
    "                    model.save(f\"model-{i}-{recall_score:.4f}.hdf5\")\n",
    "                    results_df.loc[i,'Layers_norm'] = num_layers_norm\n",
    "                    results_df.loc[i,'Layers_filter'] = num_layers_filter\n",
    "                    results_df.loc[i,'num_filters'] = num_filters\n",
    "                    results_df.loc[i,'num_nodes'] = num_nodes \n",
    "                    results_df.loc[i,'initialization'] = weight_intial \n",
    "                    results_df.loc[i,'val_acc'] = history.history['val_acc'][-1]\n",
    "                    results_df.loc[i,'train_acc'] = train_acc\n",
    "                    results_df.loc[i,'percision'] = sklearn.metrics.precision_score(y_test,pred_y)\n",
    "                    results_df.loc[i,'recall'] = sklearn.metrics.recall_score(y_test,pred_y)\n",
    "                    results_df.loc[i,'f1'] = sklearn.metrics.f1_score(y_test,pred_y)\n",
    "                    print(i)\n",
    "                    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "master_rnn.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
